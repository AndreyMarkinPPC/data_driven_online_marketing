[
["index.html", "A short Guide to Data-Driven Advertising Chapter 1 Motivation and Structure", " A short Guide to Data-Driven Advertising Florian Teschner 2017-04-17 Chapter 1 Motivation and Structure “Something is better than nothing. But is there anything in it for me?” We live in a connected world. No matter which website we visit, which app we use and which people we interact with: We leave a digital footprint. Day by day, there is more behavioral data created and it often makes using the internet more comfortable. Here is an example: Netflix concludes from user data which movies we like and subsequently optimizes which videos are suggested to us individually. Google individualizes search results and advertisers measure the effect of ad impressions on purchase probability. Tracking data helps companies to better understand consumer behavior and to customize their services. Moreover, it allows them to better target potential customers. This little book tries to detail how tracking in web currently works, how advertisers use the tracking data to target consumers and finally analyses the question if targeting helps advertisers to improve their marketing ROI. The book is written for the data-driven marketeer, to show the benefits and drawbacks of the current practice. The book is organized as follows; first it details the basic mechanism of real-time advertising, followed by a more detailed view on how online targeting data is gathered. It tries to show that targeting data is rather crude in most cases. Taking the view of a marketeer it analyses the effect of targeting on marketing ROI. Finally, it questions the current (mis-)use of tracking data in order to achieve higher sales and better ROI. "],
["introduction.html", "Chapter 2 Introduction 2.1 Real-time Ad buying", " Chapter 2 Introduction One of the key trends in the advertising industry is (digital) data-driven marketing. The whole thing starts with massive, passive data collection. No matter which website we visit or which app we use: We leave a digital footprint. These footprints are compiled for individual users and form so-called user profiles. A set of similar profiles is then aggregated to user segments, which aim at describing a homogeneous set of users with similar preferences and interests. In order to improve marketing activities, companies use these user segments to show them advertisements fitting the user’s interest. 2.1 Real-time Ad buying auction-based ad buying on impression level. "],
["basics-gathering-data.html", "Chapter 3 Basics - Gathering Data 3.1 The concept of Pixels 3.2 How does the German tagging landscape look like? 3.3 Utilization of pixel data 3.4 Consequences for security &amp; e-commerce 3.5 Design of a deliberate data strategy: 3.6 Gathering data to create user profiles 3.7 What is data onboarding? 3.8 What is look-a-like modelling?", " Chapter 3 Basics - Gathering Data We live in a connected world. No matter which website we visit, which app we use and which people we interact with: We leave a digital footprint. Day by day, there is more behavioral data created and it often makes using the internet more comfortable. Here is an example: Netflix concludes from user data which movies we like and subsequently optimizes which videos are suggested to us individually. Google individualizes search results and advertisers measure the effect of ad impressions on purchase probability. Tracking data helps companies to better understand consumer behavior and to customize their services. 3.1 The concept of Pixels User behavior on websites and platforms is mostly captured via tracking pixels (cookies). A “pixel” or “TAG” is a small piece of software that is loaded in the background of a website to collect information (undetected), about users and their behavior on the website. But sometimes these TAGs are forgotten and live on as tiny pieces of code somewhere in the depths of a website. In this case they keep collecting data - mostly unwittingly to the website owner. The control over these code snippets is often spread over multiple divisions and partners of a company (e.g. IT, website UX (creative agency), marketing (media agency) and customer relations (social media agency)). As a consequence, nobody is really responsible. But especially in this area it is important to know who is actually getting which information. Here is an example in the fashion industry: After the end of a joint campaign, an external data provider keeps collecting data about the website’s users (who are attractive for other brands) via the campaign TAG that was implemented to measure effects of the campaign. Without the website holder’s knowledge, these user profiles could be sold e.g. to a premium timepiece company or even to a competitor from the fashion industry to make a competitor’s campaign more efficient. The sad truth is that many companies neither know that many of these (external) code snippets still lie dormant on their websites nor for which party these TAGs collect data and which value this data could have for both the own company’s marketing and for other companies. Companies often lack a holistic, deliberately implemented data strategy, although answers to such questions are decisive for companies from many perspectives. 3.2 How does the German tagging landscape look like? For my job, I examined the 350 most important German websites over the past couple weeks to learn more about the current tagging landscape. The analysis shows that the average German website has 16 external pixels implemented with the range being very wide reaching from almost 40 pixels in the tourism industry to just 2 in the pharmaceutical industry. As a general pattern we see that industries dealing with sensitive data (e.g. banks and insurances) tend to have fewer pixels on their websites. The distribution of single providers is interesting as well. Among analytics providers there is a clear order of use: Google Analytics can be found on 47% of all websites, followed by Adobe with Omniture (11%), Webtrekk (10%) and Piwik (4%) take 3rd and 4th position. 3.3 Utilization of pixel data Providers whose pixels/TAGs are implemented on multiple websites can track user behavior across the different websites to create detailed user profiles. These profile segments are very useful from the advertiser’s perspective because they enable them to address members of their target group in a favorable environment. But website operators are often left out when it comes to monetize the data. From the website operators’ perspective it is therefore important to check whether data from external providers is used for such profiling and if so, how much their data is worth in this pool of profiles. Current rates for user profile data depend on the quality of the data and range from 1€to 5€ CPM based on impressions. For a website with a lot of traffic and detailed user profiles, the value of these data can add up to 100.000€ per month. 3.4 Consequences for security &amp; e-commerce Another problem occurs in the area of website security and user experience. As soon as unsafe pixels (http connection) are implemented into “safe”websites (https connection), the whole online traffic is no longer encoded. Some browsers (e.g. Internet Explorer) display security warnings in such cases. These security warnings interfere with the user and can lead to user aborting his/her online purchase. Thus, the implementation of unsafe TAGs has a direct effect on the user experience and thereby also an indirect effect on conversion rates or sales. Most TAGs are invocable as both https and http TAGs. 3.5 Design of a deliberate data strategy: The first step is to get an overview of the owned platforms and to analyze the pixels that are used on them. The second step is the ongoing control of the implemented pixels and their regular check regarding necessity and use. So-called “TAG Management Systems” offered by Google and Adobe help to do that. These systems are especially useful to avoid time consuming agreements between IT and marketing. It is not surprising that such systems are widely spread already, they can be found on 35% of all websites. But companies using such systems are not fundamentally protected against mistakes. Illustrative view on implemented pixels It is the aim of such systems to administrate and control all external pixels at one place. But there are a couple of websites where pixels are implemented outside the “Tag Management System” and thereby undermine the idea of the system. For example, figure 4 shows that the Google Tag Manager is implemented, however many pixels are loaded directly from the main website. So the arrows point from the actual website in purple directly to the pixels in blue. Considering the abundance and the complexity of the snares it is to be asked why so many pixels are still implemented and used. The benefit seems to exceed the costs by far. That mainly comes from a more efficient ad delivery because of carefully built user segments which can be addressed via targeting. On the consumer side, this segmentation usually happens on Data Management Platforms (DMP). In our survey, pixels of DMPs are found on 50% of the analyzed websites. Some DMPs also offer to advertisers to buy cookie data from third parties and to deliver ads on an individual basis. 3.6 Gathering data to create user profiles How does data collection in the web work? What is a typical data sources 3.7 What is data onboarding? 3.8 What is look-a-like modelling? Lookalike modelling is “finding new people who behave like current customers – or like a defined segment. The topic is hot, but there’s deliberate mystery about the method and its accuracy” [theguardian.com] The following section, will demystify the method and bring some science to it. The basic idea is to find users who belong to a defined audience. Starting from a seed audience such as users who already converted or users that have state to belong to an audience through an survey, the idea is to train a statistical model to identify which user actions separate that user cluster from other users. Let’s have a look at a simple example to predict a user’s gender. The data usually used to create the lookalike models consists of the websites a user has visited in the past and one outcome variable. In this example that outcome variable is the user’s gender. [image] That information is fed into a various algorithms to train models on the data. One such model can be a tree based model. The following chart exemplifies such a “Tree” model. As basis we used 20000 users and just 4 websites to classify users depending on their gender. The websites are illustrated as boxes in the middle. In the first step the model tells us that if a user visited chip.de (N&gt;=1) he is with a probability of 65% male. As a more complex example down the tree is if a user did not visit chip.de but gesuende.de she is with 73% female. That tree gets more complex if users can be segmented using more websites. Example tree-algorithm to decide on user attribute Model accuracy describes how well the model classifies users. As a simple measure we take the prediction accuracy defined as the number of correctly classified users as a percentage of all classified users. For the previous example of predicting a user’s gender we see that the accuracy is a function of how many websites are used. In the example chart 62% of all users are correctly classified by just using 20 randomly selected websites as data source. That number increases to roughly 73% when the model has access to 500+ websites. After that, additional websites do not add value. Prediction accuracy dependent on website universe • How accurate are Lookalike models? What are the key factors? o What is the effect of the number of websites used to measure user behavior? o What is the effect of different Machine Learning algorithms? o What is the effect the selection of websites used to measure user behavior? o What is the minimum number of users needed to create models? o Which attributes can reliably been used to create models? Prediction accuracy dependent on the share of users classified Find attributes or features of (converting) users Instead of looking for the ideal user, we look for the ideal behaviour – activities and interests that indicate a person will convert, regardless of what their profile may look like. Try to find that users with these features/behavior in an audience in order to reach the users with the same behavioral pattern. look alike act alike "],
["targeting.html", "Chapter 4 Targeting 4.1 Targeting - a look at ad effectiveness 4.2 Targeting - exemplified with Google Adwords 4.3 Targeting - the increment matters", " Chapter 4 Targeting One of the key trends in the advertising industry is (digital) data-driven marketing. The whole thing starts with massive, passive data collection. No matter which website we visit or which app we use: We leave a digital footprint. These footprints are compiled for individual users and form so-called user profiles. A set of similar profiles is then aggregated to user segments, which aim at describing a homogeneous set of users with similar preferences and interests. In order to improve marketing activities, companies use these user segments to show them advertisements fitting the user’s interest. To be more concrete; users visiting car sites are more likely to see BWM or Daimler-Benz ads. So far this sounds great. It is a win-win situation, advertisers reach an audience with an interest in their product and users see ads that mirror their interest. Well the key is; advertisers want to increase their advertising effectiveness. Let’s have a look at how advertising is measured. Let’s exemplify it by looking at the advertising for a car with two segments; a segment of car enthusiasts and a general population segment. Both segments receive the same ad. An example of two user segments The most common (and maybe intuitive) way to look at advertising effectiveness is by calculating the cost per order/conversion. The calculation is straight forward. The cost per order is: the ad spend by user segment divided by the number of orders generated in the user segment. As segments are generated depending on the user interests, it is fair to assume that users have different base probabilities of buying the item (car) in question. User segment #User Base probability Ad uplift Ad Spend #Orders Cost per order Car enthusiast 100 1.5% 0.50%pp 10 000 € 2 5 000 € No Targeting 100 1.0% 0.50%pp 10 000 € 1.5 6 667 € Given the example, a naive marketeer concludes that targeting the car enthusiast segment is more effective as the cost per order is 1 667€ lower. However that is only the easy way of calculating it. What you as a marketeer really want to measure is the incremental uplift. The uplift describes the increase in purchase probability when showing the user ads. The incremental uplift is only measurable by running (controlled) experiments, in which you measure the increase of sales in the segment exposed to ads to the “same” segment not exposed to ads. Such controlled experiments are rarely done on a segment basis. In this example, both marketing activities have the same incremental uplift in purchase probability (0.5%pp). Hence, if you ignore that the marketeer have to buy targeting data on top of her media spend, the incremental uplift is the same for both groups as is the return for the ad investment. This brings us to the key message; current systems (ad serving in particular) are designed in a way that they are basically unable to capture users base purchase probability. Hence, they are deeply flawed and purposefully biased towards benefiting targeted ad buying. 4.1 Targeting - a look at ad effectiveness In the last chapter, I discussed how the current digital measurement approach is biased towards targeted ad buying. The key reason is that ad effectiveness is calculated on a cost per order/conversion basis. As particular user segments -which are addressed with digital targeting- have a high base purchase probability, the segment looks more responsive. Due to insufficient experimentation and proper measurement, the budget optimization tries to reach users who are likely to order/“convert” anyway. Hence, it ignores incremental effects and instead focuses on overall purchase probabilities, mixing base purchase probability and ad-driven incremental effects. Let’s go back a step and have a look at two scenarios. Scenario A is taken from the previous post. There are two user segments; a segment of car enthusiasts and a general population segment. Both segments receive the same ad. The car enthusiasts (Segment 1) has a higher base probability of purchasing a car. As discussed that leads to a lower cost per order for that group and sub-sequentially to a misguided budget shift. Example of ad effectiveness Let’s add the second Scenario B. In this case the incremental effect of showing an ad is higher in the the general population segment (Segment 2) compared so Segment 1. It might be that the ad communicates a new feature (Matrix LED lights, head-up-display,…) that Segment 1 was not aware of before. In that scenario a budget shift would increase the overall sales. However, that budget shift would only be realized if the incremental effect would have been measured. The cost per order is still lower for Segment 1. To sum up, current systems try hard to minimize the cost per conversion. This leads to showing ads to users who are likely to convert anyway, if possible. From a theoretical point of view, (e.g. assuming that targeting works perfectly) that leads to an overall decreasing ad effectiveness. 4.2 Targeting - exemplified with Google Adwords In the last two chapters, I tried to discuss how measurement and false metrics drive optimization towards low hanging fruits and in the end degrade ad effectiveness. I would like to follow up with a a short example of how the issue extends into the paid search (e.g. Google Adwords) channel. Search traffic is split up into two parts; the organic, free traffic and paid traffic. The following chart illustrates the difference. Paid and orangic search traffic For the paid part advertisers are allowed buy individual keywords (search terms that users might type in) on a cost per click basis. Each keyword sells for a different cost per click depending on competition, relevancy and click-through-rate. Usually the advertiser picks the keywords with lowest cost per click. Keywords can be clustered into at least two broad categories: Brand and Generic keywords. Brand keywords contain the brand name of the advertiser while generic keywords do not and just relate to an (unspecific) inquiry. From a consumer journey perspective, generic terms are searched in the beginning of a purchase decision process, while brand keywords are used towards the end. Generally, that means that brand keywords have a much lower cost per click than generic keywords. Hence advertisers buy brand keywords all the time. 4.3 Targeting - the increment matters It is save to assume that advertisers are organically listed in the top positions for brand keywords, while they might not even appear on the first results page for generic terms. As paid advertising is shown above the organic section a majority of users click on the paid links. Hence, an advertiser buying her “own” brand keywords is cannibalizing her organic, free traffic with paid search traffic. The following chart illustrates this: The share of incremental traffic for buying generic keywords is much higher compared to buying brand keywords. Using proper experimentation one can figure out what that share is and calculate the cost per incremental visitor as a more reliable metric. The questions arises why this is usually not done. I see three main reasons: (i) incentives, (ii) additional work and (iii) Google’s quality score. For the team managing paid search it is much easier to communicate low costs per click (CPC) readily delivered by the system compared to relying on a derived metric that needs constant experimentation. As most people are familiar with CPCs they are also commonly used to benchmark channels and/or team performance. Hence increasing CPCs is a difficult story to tell. A part of the cost per click on a keyword is determined by the quality score, Google assigns to an individual advertiser. The lower the score the higher the cost an advertiser has to pay per click. Even though Google does not openly discuss the algorithm behind the quality score, it is pretty clear that the Click-through-Rate (which is higher for brand keywords) improves the score. As a result of their policy brands are forced or at least given a decent incentive to buy their brand keywords. To sum up, in the paid search channel, the system is designed in a way to ensure that brands buy keywords for users who are very likely to click/convert anyways. "],
["targeting-attribution.html", "Chapter 5 Targeting - Attribution 5.1 Measuring the effect on sales", " Chapter 5 Targeting - Attribution why it is important to measure the increment, and how that can be done — sometimes. 5.1 Measuring the effect on sales power caluclation given some numbers "],
["buying-data-from-third-parties.html", "Chapter 6 Buying Data from third parties 6.1 Selecting Data Partners 6.2 Data: A Market for Lemons 6.3 Evaluating data providers", " Chapter 6 Buying Data from third parties Short intro about data buying, how that fits into the ecosystem 6.1 Selecting Data Partners Reach and Quality charts. 6.2 Data: A Market for Lemons Reach = Profit –&gt; reason to evaluate data on a continuous basis 6.3 Evaluating data providers "],
["final-words.html", "Chapter 7 Final Words", " Chapter 7 Final Words I hope you enjoyed reading the book and took some value from it. "],
["references.html", "Chapter 8 References", " Chapter 8 References "]
]
